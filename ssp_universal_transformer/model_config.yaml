model_params:
    embed_size: 128
    transformer_layers: 6
    attention_heads: 4
    residual_dropout: 0.1
    attention_dropout: 0.1
    conv_dropout: 0.2
    mask_attention: False
    optimizer: Nadam 
run_params:
    epochs: 15
    batch_size: 16
